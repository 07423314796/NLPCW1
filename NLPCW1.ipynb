{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/tcgSCNKhAXrxFVcMuBgC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/07423314796/NLPCW1/blob/main/NLPCW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "thncDUQYntNy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset:\n",
        "Data=pd.read_csv(r\"/olid-training-v1.0.tsv\", index_col = False,sep=\"\\t\")\n"
      ],
      "metadata": {
        "id": "WdSzUmP5nyCi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "New_data=pd.DataFrame(Data)"
      ],
      "metadata": {
        "id": "ifzMGS5EosfL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subtask_a groups: Output: array(['OFF', 'NOT'], dtype=object)\n",
        "New_data['subtask_a'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_35-iWuKovJJ",
        "outputId": "e964d6d0-dc62-4384-a3ad-abbea250945f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, 'IND', 'OTH', 'GRP'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#subtask_b groups: output: array(['UNT', 'TIN', nan], dtype=object)\n",
        "New_data['subtask_b'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RElTxdauoxk4",
        "outputId": "72544cd9-80fc-4c9d-d10d-a61cf7569958"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['UNT', 'TIN', nan], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Subtask_c Group: output:array([nan, 'IND', 'OTH', 'GRP'], dtype=object)\n",
        "New_data['subtask_c'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wYjqkPWo4Gb",
        "outputId": "86c87451-bdd6-4b30-8329-33c9e45420fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, 'IND', 'OTH', 'GRP'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "i-w6bCyfo4pO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing :\n",
        "import re\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w9lZpiio9Ux",
        "outputId": "daed079e-d204-4650-d071-63425969fac2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the text data\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X.apply(self._preprocess)\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        # Remove non-alphabetic characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Tokenize and remove stop words\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word.lower() not in self.stop_words]\n",
        "        # Join tokens back into a string\n",
        "        return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "1ydOuxNSpFXt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text preprocessing\n",
        "New_data['clean_text'] = TextPreprocessor().fit_transform(New_data['tweet'])\n"
      ],
      "metadata": {
        "id": "WmDPhBOWpKcr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and labels\n",
        "X = New_data['clean_text']\n",
        "y_a = New_data['subtask_a']\n",
        "y_b = New_data['subtask_b'].fillna('NONE')\n",
        "y_c = New_data['subtask_c'].fillna('NONE')\n"
      ],
      "metadata": {
        "id": "Rl4oIRLbpNXE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X, y_a, test_size=0.2, random_state=42)\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X, y_b, test_size=0.2, random_state=42)\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_c, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2qhwiBwfpP0g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classification algorithms\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "KDPn1dCapTCS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate models\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
        "    results = {}\n",
        "    for name, clf in classifiers.items():\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        results[name] = {'accuracy': accuracy, 'f1_score': f1}\n",
        "        print(f\"{name} - Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "B-6rniF6pWba"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate models for each subtask\n",
        "print(\"Subtask A Results:\")\n",
        "results_a = train_and_evaluate(X_train_a, X_test_a, y_train_a, y_test_a)\n",
        "print(\"\\nSubtask B Results:\")\n",
        "results_b = train_and_evaluate(X_train_b, X_test_b, y_train_b, y_test_b)\n",
        "print(\"\\nSubtask C Results:\")\n",
        "results_c = train_and_evaluate(X_train_c, X_test_c, y_train_c, y_test_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skz4c_G5pZdw",
        "outputId": "05292175-9144-473c-fce8-239aea1f86ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask A Results:\n",
            "Logistic Regression - Accuracy: 0.76, F1 Score: 0.73\n",
            "Naive Bayes - Accuracy: 0.70, F1 Score: 0.63\n",
            "Random Forest - Accuracy: 0.75, F1 Score: 0.73\n",
            "Support Vector Machine - Accuracy: 0.75, F1 Score: 0.72\n",
            "\n",
            "Subtask B Results:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 0.74, F1 Score: 0.70\n",
            "Naive Bayes - Accuracy: 0.68, F1 Score: 0.58\n",
            "Random Forest - Accuracy: 0.73, F1 Score: 0.69\n",
            "Support Vector Machine - Accuracy: 0.73, F1 Score: 0.68\n",
            "\n",
            "Subtask C Results:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 0.73, F1 Score: 0.66\n",
            "Naive Bayes - Accuracy: 0.70, F1 Score: 0.58\n",
            "Random Forest - Accuracy: 0.73, F1 Score: 0.67\n",
            "Support Vector Machine - Accuracy: 0.72, F1 Score: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to predict the offensive language. (SUB Task A, B ,C covered)\n",
        "\n",
        "def detect_offensive_language(text):\n",
        "  # Preprocess the text\n",
        "  # Convert the text into a Pandas Series for preprocessing\n",
        "  text_series = pd.Series([text])\n",
        "  clean_text = TextPreprocessor().fit_transform(text_series)[0] # Pass the Series to fit_transform\n",
        "\n",
        "  # Classify the text for subtask A\n",
        "  pipeline_a = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer()),\n",
        "      ('clf', LogisticRegression())\n",
        "  ])\n",
        "  pipeline_a.fit(X_train_a, y_train_a)\n",
        "  subtask_a_prediction = pipeline_a.predict([clean_text])[0]\n",
        "\n",
        "  # If the text is not offensive, return None\n",
        "  if subtask_a_prediction == 'NOT':\n",
        "    return None\n",
        "\n",
        "  # Classify the text for subtask B\n",
        "  pipeline_b = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer()),\n",
        "      ('clf', MultinomialNB())\n",
        "  ])\n",
        "  pipeline_b.fit(X_train_b, y_train_b)\n",
        "  subtask_b_prediction = pipeline_b.predict([clean_text])[0]\n",
        "\n",
        "  # Classify the text for subtask C\n",
        "  pipeline_c = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer()),\n",
        "      ('clf', RandomForestClassifier())\n",
        "  ])\n",
        "  pipeline_c.fit(X_train_c, y_train_c)\n",
        "  subtask_c_prediction = pipeline_c.predict([clean_text])[0]\n",
        "\n",
        "  # Return the results\n",
        "  return {\n",
        "      'offensive': True,\n",
        "      'insult_type': subtask_b_prediction,\n",
        "      'target': subtask_c_prediction\n",
        "  }\n",
        "\n",
        "# Example usage\n",
        "text = input(\"Enter a text: \")\n",
        "result = detect_offensive_language(text)\n",
        "\n",
        "# Check if result is not None before accessing keys\n",
        "if result is not None:\n",
        "  if result['offensive']:\n",
        "    print(f\"The text is offensive.\")\n",
        "    print(f\"Insult type: {result['insult_type']}\")\n",
        "    print(f\"Target: {result['target']}\")\n",
        "  else:\n",
        "    print(\"The text is not offensive.\")\n",
        "else:\n",
        "    print(\"The text is not offensive.\") # Handle the case where result is None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1hjZx_fqoIW",
        "outputId": "f385317a-64e9-4654-a862-eca7553e469c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: @USER ur so straight forword manðŸ‘Œ i saw u in dance dewwane and ur just talk free ky ap kitnay porrany ho industry mein and i really like ur this\n",
            "The text is not offensive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MwGLlm2tuhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}